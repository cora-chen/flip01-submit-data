%=================================================================
\vskip 0.9 cm%
\section{INTRODUCTIONS}\label{sec-intro}
\vskip 0.5 cm%

\setlength{\parindent}{2em}


Asks you to predict the category of a dish's cuisine given a list of its ingredients. Test and submit the results to see the experimental score.


  \begin{tabular}{ c | c | c }
    \toprule
    ID &  Cuisine    & Ingredients      \\
    \midrule
    10259 &  greek & romaine lettuce, black olives, grape tomatoes...\\
    25693 & southernus &plain flour, ground pepper, salt, tomatoes, g...\\
    20130  & filipino &eggs, pepper, salt, mayonaise, cooking oil, g... \\
    22213 & indian &water, vegetable oil, wheat, salt]\\
    13162& indian& black pepper, shallots, cornflour, cayenne pe...\\
    \bottomrule
\end{tabular}

\vskip 0.9 cm%
\section{DATA ANALYSIS} \label{sec-preliminaries}
\vskip 0.5 cm%
\setlength{\parindent}{2em}

To better process the data, we need to do the following:


\begin{enumerate}[1]
 
  \item Count the total data of training set and test set.\\
  train shape: 39774\\
  test shape: 9944
  \item
        Maximum Number of Ingredients in a Dish:  65
        \item
        Minimum Number of Ingredients in a Dish:  1\\
        train: 8529\\
        test: 3310
\end{enumerate}



\vskip 0.5cm%
\section{NLP ANALYSIS} \label{sec-method}
\vskip 0.5 cm%
\setlength{\parindent}{2em}

\begin{enumerate}[1]
  \item
  Model of TF-IDF algorithm:
  \vskip 0.5 cm%
  
  \begin{centering}
   
  $ TF-IDF (d, w) = TF (d, w) *IDF(w)$
  
  
  \end{centering}
  \vskip 0.5 cm%
  \begin{enumerate}
  
  \item
  $TF(d,w)$ $\Leftrightarrow$ Frequency of occurrence of w in document d.
  \item
  $IDF(w) = log\frac{N}{N(w)}$
  \item
  N $\Leftrightarrow$ The total number of documents in a corpuss.
  \item
  N(w)$\Leftrightarrow$ How many documents does the w appear in.
  \end{enumerate}
  \item
            The counting matrix of words is converted to TF-IDF representation, and then normalized.
            \item
            Scikit-learn provides a TfidfVectorizer class, which has the ability to remove common stop words (like a, the, and, or).
            \item
            TF-IDF tends to filter out common words and retain important words.
            
\end{enumerate}
\newpage

\vskip 0.9 cm%
\section{MODELING} \label{sec-experiment}
\vskip 0.5 cm%


\begin{enumerate}[1]
    
  \smallskip
  \item
  Logistic Regressio:\\
  Random seeds are not fixed and generate random sequences.\\
  Use the logistic regression model in sklearn.\\
  Score: 0.787711182622687.
  \item
  \smallskip
  Ensemble Model:\\
  Ensemble in Sklearn is called to integrate the two classifiers, logistic regression and SVM.\\
in the way of soft voting, to show the accuracy.\\ 
Score: 0.8119469026548672. \\ 
By comparing the accuracy of the two models, we found that the integrated accuracy is higher than that of a single classifier.
     
\end{enumerate}

\vskip 0.5 cm%
\section{SUMMARY} \label{sec-experiment}
\vskip 0.5 cm%

\begin{enumerate}[1]

  \item
  \smallskip
  \large
  {
  Output Dataframe:
  }
  
\end{enumerate}


\vspace{.5cm}
\centering
\begin{tabular}{ c | c | c }
\toprule
&  ID    & Cuisine \\
\midrule
0 &  18009    &  british  \\

1 &  28583    &  southern\_us \\

2 &   41580    &  italian \\
3 &   29752    &  cajun\_creole \\
4 &   35687    &  italian    \\
5 &   38527    &  southern\_us\\
\bottomrule
\end{tabular}
\vskip 0.3 cm%
\begin{enumerate}[1]
\item
        Dishes can contain a variety of ingredients, and the same ingredients may vary in number and number, so the integredients need to be filtered.
        \item
        KNN mainly depends on the surrounding limited adjacent samples, rather than on the method of discriminating class domain to determine the category.
        \item
        KNN basically does not learn, resulting in a slower prediction speed than logistic regression and other algorithms.
\end{enumerate}